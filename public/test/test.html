<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–£–º–Ω–∞—è –¥–∏–∫—Ç–æ–≤–∫–∞ (–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è)</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            background-color: #f0f2f5;
            margin: 0;
            color: #333;
        }
        h1 {
            color: #1a1a1a;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            cursor: pointer;
            margin-bottom: 20px;
            border: none;
            border-radius: 8px;
            background-color: #007bff;
            color: white;
            transition: background-color 0.3s ease;
        }
        button:hover {
            background-color: #0056b3;
        }
        #transcript {
            width: 80%;
            max-width: 600px;
            min-height: 150px;
            padding: 15px;
            border: 1px solid #ccc;
            border-radius: 8px;
            font-size: 18px;
            line-height: 1.6;
            background-color: #fff;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        #status {
            margin-top: 15px;
            font-style: italic;
            color: #666;
        }
    </style>
</head>
<body>

<h1>–£–º–Ω–∞—è –¥–∏–∫—Ç–æ–≤–∫–∞ (–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è)</h1>
<p>–ì–æ–≤–æ—Ä–∏—Ç–µ –∫–æ–º–∞–Ω–¥—ã ("—Ç–æ—á–∫–∞", "–æ—Ç–º–µ–Ω–∞" –∏ —Ç.–¥.) –ø—Ä—è–º–æ –≤ –ø–æ—Ç–æ–∫–µ —Ä–µ—á–∏.</p>
<button id="startButton">–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å</button>
<div id="transcript" contenteditable="true" placeholder="–ù–∞—á–Ω–∏—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å..."></div>
<div id="status"></div>

<script>
    const startButton = document.getElementById('startButton');
    const transcriptDiv = document.getElementById('transcript');
    const statusDiv = document.getElementById('status');

    if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
        statusDiv.textContent = '–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Web Speech API. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Chrome –∏–ª–∏ Edge.';
        startButton.disabled = true;
    } else {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();

        let finalTranscript = '';

        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'ru-RU';

        // –û–±—ä–µ–∫—Ç —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ –∏ –∏—Ö –∑–∞–º–µ–Ω–∞–º–∏.
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –∑–∞–º–µ–Ω—ã.
        const commandProcessors = {
            // –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–æ–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            '—Ç–æ—á–∫–∞': { regex: /\s+—Ç–æ—á–∫–∞/gi, replacement: '.' },
            '–∑–∞–ø—è—Ç–∞—è': { regex: /\s+–∑–∞–ø—è—Ç–∞—è/gi, replacement: ',' },
            '–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫': { regex: /\s+–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫/gi, replacement: '?' },
            '–≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫': { regex: /\s+–≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫/gi, replacement: '!' },
        };
        const undoCommand = '–æ—Ç–º–µ–Ω–∞';

        const applyUndo = () => {
            if (finalTranscript.length === 0) return;
            // –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –ø—É—Å—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            let words = finalTranscript.trim().split(' ').filter(word => word.length > 0);
            if (words.length > 0) {
                words.pop(); // –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
                finalTranscript = words.join(' ') + (words.length > 0 ? ' ' : '');
            }
        };

        recognition.onstart = () => {
            statusDiv.textContent = '–°–ª—É—à–∞—é... –ì–æ–≤–æ—Ä–∏—Ç–µ –≤ –º–∏–∫—Ä–æ—Ñ–æ–Ω.';
            startButton.textContent = '–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å';
            transcriptDiv.focus();
        };

        recognition.onend = () => {
            statusDiv.textContent = '–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.';
            startButton.textContent = '–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å';
            // –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º, –µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            transcriptDiv.textContent = finalTranscript;
        };

        recognition.onerror = (event) => {
            console.error('–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏:', event.error);
            statusDiv.textContent = `–û—à–∏–±–∫–∞: ${event.error}. –ù–∞–∂–º–∏—Ç–µ "–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å" —Å–Ω–æ–≤–∞.`;
            startButton.textContent = '–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å';
        };

        recognition.onresult = (event) => {
            let interimTranscript = '';
            for (let i = event.resultIndex; i < event.results.length; i++) {
                let segment = event.results[i][0].transcript;

                if (event.results[i].isFinal) {
                    // --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò ---

                    // 1. –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–∫–∞–∑–∞–ª–∏ "–æ—Ç–º–µ–Ω–∞"
                    const undoRegex = new RegExp(undoCommand, 'gi');
                    const undoCount = (segment.match(undoRegex) || []).length;

                    // –£–¥–∞–ª—è–µ–º —Å–∞–º–æ —Å–ª–æ–≤–æ "–æ—Ç–º–µ–Ω–∞" –∏–∑ —Ñ—Ä–∞–∑—ã
                    segment = segment.replace(undoRegex, '').trim();

                    // 2. –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—ã –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                    for (const key in commandProcessors) {
                        segment = segment.replace(commandProcessors[key].regex, commandProcessors[key].replacement);
                    }

                    // 3. –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                    if (segment) {
                        let textToAdd = segment;
                        // –ê–≤—Ç–æ–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è
                        if (finalTranscript.length === 0 || finalTranscript.endsWith('. ') || finalTranscript.endsWith('? ') || finalTranscript.endsWith('! ')) {
                            textToAdd = textToAdd.trimStart();
                            textToAdd = textToAdd.charAt(0).toUpperCase() + textToAdd.slice(1);
                        }
                        finalTranscript += textToAdd + ' ';
                    }

                    // 4. –í—ã–ø–æ–ª–Ω—è–µ–º "–æ—Ç–º–µ–Ω—É" –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑
                    for (let j = 0; j < undoCount; j++) {
                        applyUndo();
                    }

                } else {
                    interimTranscript += segment;
                }
            }

            transcriptDiv.textContent = finalTranscript + interimTranscript;
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        };

        startButton.addEventListener('click', () => {
            if (startButton.textContent === '–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å') {
                finalTranscript = '';
                transcriptDiv.textContent = '';
                recognition.start();
            } else {
                recognition.stop();
            }
        });
    }
</script>

</body>
</html>

<div class="chat-messages" id="chatMessages">
    <div class="message user" id="msg-1750665304738-3dahxq" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
        <div class="message-content">
            <div class="message-header">
                <span>–¢—ã</span>
                <span>10:55</span>
            </div>
            <div class="message-text"><pre class="language-css" tabindex="0"><code class="language-css">&lt;p&gt;–ö–∞–∫–∏–µ –∏–∑ –º–æ–¥–µ–ª–µ–π<span class="token punctuation">,</span> –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏ –ø–æ–º–æ—â–∏ –Ω–∞ RTX 4090 –∏ 128GB RAM –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥–æ–π–¥—É—Ç –¥–ª—è –∑–∞–¥–∞—á–∏ - fine-tuning <span class="token punctuation">(</span>–Ω–∞ —ç—Ç–æ–π –∂–µ –º–∞—à–∏–Ω–µ<span class="token punctuation">)</span> –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å –æ—á–µ–Ω—å —Ç–æ—á–Ω—ã–º –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ —á—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç —á–µ–ª–æ–≤–µ–∫<span class="token punctuation">,</span> –∫–∞–∫ —Å–µ–±—è –ø–æ–≤–µ–¥—ë—Ç<span class="token punctuation">,</span> —Å –±—É–¥—É—â–µ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ—Ç–æ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç–æ–≥–æ<span class="token punctuation">,</span> –∫–∞–∫ –æ–Ω –ø–æ–≤–µ–¥—ë—Ç —Å–µ–±—è<span class="token punctuation">,</span> –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ<span class="token punctuation">,</span> –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å–¥–µ–ª–∞–µ–º<span class="token punctuation">,</span> –∫–∞–∫ –∑–∞–¥–∞–¥–∏–º –≤–æ–ø—Ä–æ—Å –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ<span class="token punctuation">,</span> —Ç–µ–º —Å–∞–º—ã–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–≤–∏–¥–µ—Ç—å –∏ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –≤ —Å–≤–æ—é —Å—Ç–æ—Ä–æ–Ω—É –ø–æ–≤–µ–¥–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. –í–æ–ø—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ –∏ –æ–±—ã—á–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ<span class="token punctuation">,</span> —Ç–∞–∫ –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π<span class="token punctuation">,</span> –ª–∏—á–Ω—ã—Ö.
–¢–∞–∫–∂–µ —Å–∫–∞–∂–∏ –∫–∞–∫ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥–æ–π—Ç–∏ –∫ —Ç–∞–∫–æ–º—É —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É<span class="token punctuation">,</span> —á—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ã–ª —Å–∞–º—ã–π –ª—É—á—à–∏–π –∏ –º—ã –Ω–µ –º—É—á–∏–ª–∏—Å—å –¥–æ–ª–≥–æ<span class="token punctuation">,</span> –ø—ã—Ç–∞—è—Å—å –µ–≥–æ –ø–æ–ª—É—á–∏—Ç—å.
<span class="token property">–î–∏—Å–∫–ª–µ–π–º–µ—Ä</span><span class="token punctuation">:</span> —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç<span class="token punctuation">,</span> –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–æ–∂—É —è —Å–∞–º<span class="token punctuation">,</span> –Ω–∞ —Å–µ–±–µ –∂–µ<span class="token punctuation">,</span> –∏ —ç—Ç–æ—Ç —á–µ–ª–æ–≤–µ–∫<span class="token punctuation">,</span> –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥–µ–º –≤ —Ö–æ–¥–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å<span class="token punctuation">,</span> —ç—Ç–æ —è –∂–µ —Å–∞–º.&lt;/p&gt;
</code></pre></div>
        </div>
    </div><div class="message ai" id="msg-1750665304742-c6otap" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
    <div class="message-avatar">
        <i class="custom-icon-mask"></i>
    </div>
    <div class="message-content">
        <div class="message-text"><thinking>
            –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã –ª—É—á—à–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è fine-tuning –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤, –∏–º–µ—è –≤ —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ RTX 4090 –∏ 128GB RAM. –≠—Ç–æ —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞, —Ç—Ä–µ–±—É—é—â–∞—è –∫–∞–∫ –ø–æ–¥–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏, —Ç–∞–∫ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ fine-tuning.

            <p>–Ø –º–æ–≥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å <code>Conv_Informative</code> —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è fine-tuning –∏ –æ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –ó–∞—Ç–µ–º, —Å —É—á–µ—Ç–æ–º –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —è —Å–º–æ–≥—É –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥—Ö–æ–¥—É –∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É.
            </p>
        </thinking></div>
    </div>
</div><div class="message ai tool-call" id="tool-SPEC_LLM-1750665307591" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
    <div class="message-avatar">
        <i class="fas fa-brain tool-icon-animation"></i>
    </div>
    <div class="message-content">
        <div class="message-header tool-header">
            <div class="tool-title">
                <i class="fas fa-brain"></i>
                <span>–ò—Å–ø–æ–ª—å–∑—É–µ–º Conv_Informative</span>
            </div>
            <span>10:55</span>
        </div>
        <div class="tool-description">
            Using model Conv_Informative for deep reasoning...
        </div>
        <div class="tool-visualization-container">
            <div class="spec-llm-output-container" style="line-height: 24px; color: rgb(255, 255, 255); font-size: 16px; --fade-end: 100%; --fade-start: 0%; max-height: 5000px;">
                <div class="spec-llm-content"><p>–î–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è <code>fine-tuning</code> –Ω–∞ <code>RTX 4090</code> —Å <code>128GB RAM</code>, –≤–∞–º –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ <code>open-source</code> –º–æ–¥–µ–ª–∏ –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Å–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ <code>fine-tuning</code>.</p>
                    <h2>Open-Source LLM –º–æ–¥–µ–ª–∏ –¥–ª—è Fine-Tuning –Ω–∞ RTX 4090 —Å 128GB RAM</h2>
                    <p>–•–æ—Ç—è <code>RTX 4090</code> –∏–º–µ–µ—Ç <code>24GB</code> VRAM, 128GB —Å–∏—Å—Ç–µ–º–Ω–æ–π RAM –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ <code>offloading</code> (–ø–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É GPU –∏ CPU) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.</p>
                    <ul>
                        <li><strong>Llama 2 / Llama 3 (Meta):</strong> –ú–æ–¥–µ–ª–∏ <code>Llama 2</code> –∏ <code>Llama 3</code> –æ—Ç Meta —è–≤–ª—è—é—Ç—Å—è –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è <code>fine-tuning</code> –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (<code>commercially available for free</code>) –∏ —Ö–æ—Ä–æ—à–∏–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º. <code>Llama 3</code> –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö –æ—Ç <code>8B</code> –¥–æ <code>70B</code> –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ <code>Llama 2</code> –º–æ–∂–Ω–æ <code>fine-tune</code> –Ω–∞ <code>RTX 4090</code> —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–æ–¥–µ–ª—è–º–∏, –Ω–µ –ø–æ–º–µ—â–∞—é—â–∏–º–∏—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ VRAM GPU, —Ç–∞–∫–∏—Ö –∫–∞–∫ <code>Peft</code>, <code>Bitsandbytes</code> –∏ <code>TRL</code>. <code>Llama-3 70B</code> —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.</li>
                        <li><strong>Mistral 7B:</strong> –≠—Ç–∞ –º–æ–¥–µ–ª—å —Å—Ç—Ä–µ–º–∏—Ç—Å—è –Ω–∞–π—Ç–∏ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ —Ö–æ—Ä–æ—à–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –±—ã—Å—Ç—Ä–æ–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏. <code>Mistral 7B</code> –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö.</li>
                        <li><strong>Falcon 40B:</strong> –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ò–Ω—Å—Ç–∏—Ç—É—Ç–æ–º –ò–Ω–Ω–æ–≤–∞—Ü–∏–π (TII), <code>Falcon 40B</code> –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö NLP, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞.</li>
                        <li><strong>GPT-NeoX:</strong> <code>GPT-NeoX</code> ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å-–¥–µ–∫–æ–¥–µ—Ä –Ω–∞ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π <code>GPT-3</code>. –û–Ω–∞ –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞ –∏ <code>few-shot learning</code>.</li>
                        <li><strong>DeepSeek-R1:</strong> –≠—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑.</li>
                    </ul>
                    <blockquote>
                        <p>–ò—Å–ø–æ–ª—å–∑—É—è <code>LoHan</code>, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è <code>fine-tuning</code>, –º–æ–∂–Ω–æ <code>fine-tune</code> –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ <code>175B</code> –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ <code>RTX 4090</code> —Å <code>256GB</code> –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–º—è—Ç–∏. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ <code>offloading</code> –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ <code>activation tensors</code> –Ω–∞ SSD.</p>
                    </blockquote>
                    <h2>Best Practices –¥–ª—è Fine-tuning LLM —Å —Ü–µ–ª—å—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞</h2>
                    <p>–í–∞—à–∞ –∑–∞–¥–∞—á–∞ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ <code>fine-tuning</code>.</p>
                    <h3>1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö</h3>
                    <ul>
                        <li><strong>–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:</strong> –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–≤–∞–π—Ç–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –¥–∞–Ω–Ω—ã–º. –ú–µ–Ω—å—à–∏–π, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –±–æ–ª—å—à–æ–≥–æ –∏ "—à—É–º–Ω–æ–≥–æ".</li>
                        <li><strong>–†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å:</strong> –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–∞—à –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è <code>fine-tuning</code> —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–µ–Ω –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –¥–æ–º–µ–Ω–∞ –∏ —Å–≤–æ–±–æ–¥–µ–Ω –æ—Ç —Å–º–µ—â–µ–Ω–∏–π –∏–ª–∏ –æ—à–∏–±–æ–∫.</li>
                        <li><strong>–û—á–∏—Å—Ç–∫–∞ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è:</strong> –£–¥–∞–ª—è–π—Ç–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏–ª–∏ –æ—à–∏–±–æ—á–Ω—ã–µ –∑–∞–ø–∏—Å–∏, –∏—Å–ø—Ä–∞–≤–ª—è–π—Ç–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º —á–µ—Ç–∫–æ –º–∞—Ä–∫–∏—Ä—É–π—Ç–µ —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.</li>
                        <li><strong>–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è:</strong> –£–¥–∞–ª—è–π—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (<code>PII</code>) –∏–∑ –¥–∞–Ω–Ω—ã—Ö.</li>
                        <li><strong>–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> –û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö.</li>
                        <li><strong>–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (Data Augmentation):</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —É–ª—É—á—à–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ <code>overfitting</code>.</li>
                    </ul>
                    <h3>2. –í—ã–±–æ—Ä –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏</h3>
                    <ul>
                        <li><strong>–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–¥–∞—á–∏:</strong> –í—ã–±–µ—Ä–∏—Ç–µ <code>pre-trained</code> –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–µ–π –∑–∞–¥–∞—á–µ, —á—Ç–æ–±—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ <code>fine-tuning</code>.</li>
                        <li><strong>PEFT (Parameter-Efficient Fine-Tuning):</strong> –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö GPU, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–µ—Ö–Ω–∏–∫–∏ <code>PEFT</code>, —Ç–∞–∫–∏–µ –∫–∞–∫ <code>LoRA</code> (Low-Rank Adaptation) –∏ <code>QLoRA</code> (Quantized LoRA). –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å –ª–∏—à—å –º–∞–ª—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏. <code>QLoRA</code> –ø–æ–∑–≤–æ–ª—è–µ—Ç <code>fine-tune</code> –¥–∞–∂–µ <code>70B</code> –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ <code>RTX 4090</code>.</li>
                        <li><strong>Offloading:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>offloading</code> –¥–ª—è –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ GPU –≤ CPU/SSD, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç <code>fine-tune</code> –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏, —á–µ–º –ø–æ–∑–≤–æ–ª—è–µ—Ç VRAM GPU.</li>
                    </ul>
                    <h3>3. –ú–µ—Ç–æ–¥—ã Fine-Tuning</h3>
                    <ul>
                        <li><strong>Supervised Fine-Tuning (SFT):</strong> –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ –∫–∞–∂–¥–æ–µ –≤—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏–ª–∏ –º–µ—Ç–∫–æ–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–µ—Ç–æ–∫.</li>
                        <li><strong>Reinforcement Learning from Human Feedback (RLHF):</strong> –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ —É–ª—É—á—à–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ <code>LLM</code> —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π. –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏, –∏ —ç—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è. –ó–∞—Ç–µ–º <code>LLM</code> –æ–±—É—á–∞–µ—Ç—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. <code>RLHF</code> —Ç–∞–∫–∂–µ –ø–æ–º–æ–≥–∞–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è, —Å–æ–≥–ª–∞—Å—É—è –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —ç—Ç–∏—á–µ—Å–∫–∏–º–∏ –Ω–æ—Ä–º–∞–º–∏ –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏.</li>
                        <li><strong>Human-in-the-Loop (HITL):</strong> –í–∫–ª—é—á–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —É—á–∞—Å—Ç–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å <code>fine-tuning</code> –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.</li>
                    </ul>
                    <h3>4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h3>
                    <ul>
                        <li><strong>–ò—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞:</strong> –¢—â–∞—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (<code>learning rate</code>), —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (<code>batch size</code>) –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (<code>epochs</code>), —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏.</li>
                        <li><strong>–¢–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É (<code>early stopping</code>) –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (<code>learning rate scheduling</code>) –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è <code>overfitting</code> –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è.</li>
                        <li><strong>–°–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Mixed Precision):</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.</li>
                    </ul>
                    <h3>5. –û—Ü–µ–Ω–∫–∞ –∏ –∏—Ç–µ—Ä–∞—Ü–∏—è</h3>
                    <ul>
                        <li><strong>–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:</strong> –†–µ–≥—É–ª—è—Ä–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–π—Ç–µ –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç—Ä–∏–∫–∏ –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏.</li>
                        <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:</strong> –û—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –ø–æ—Ç–µ—Ä—é (<code>loss</code>) –∏ —Ç–æ—á–Ω–æ—Å—Ç—å (<code>accuracy</code>) –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ï—Å–ª–∏ –ø–æ—Ç–µ—Ä—è –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∞–µ—Ç—Å—è. –ï—Å–ª–∏ –æ–Ω–∞ –ø–∞–¥–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–æ, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–æ –æ–±–æ–±—â–µ–Ω–∏—è (<code>overfitting</code>).</li>
                        <li><strong>–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥:</strong> –ü–æ–≤—Ç–æ—Ä—è–π—Ç–µ —Ü–∏–∫–ª—ã –º–µ–∂–¥—É <code>prompt engineering</code>, <code>fine-tuning</code> –∏ –æ—Ü–µ–Ω–∫–æ–π –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∂–µ–ª–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.</li>
                    </ul>
                    <h3>–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏:</h3>
                    <ul>
                        <li><strong>–≠—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è:</strong> –ü—Ä–∏ <code>fine-tuning</code> –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫—Ä–∞–π–Ω–µ –≤–∞–∂–µ–Ω —ç—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö <code>fine-tuning</code>. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ —É—Å–∏–ª–∏–≤–∞–µ—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å.</li>
                        <li><strong>–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö:</strong> –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã, –∫–∞–∫ <code>DP-Stochastic Gradient Descent (DP-SGD)</code> –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—É–º–∞ –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –±–µ–∑ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</li>
                        <li><strong>Prompt Engineering:</strong> –ü–æ–º–∏–º–æ <code>fine-tuning</code>, <code>prompt engineering</code> –º–æ–∂–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –æ—Ç–≤–µ—Ç—ã <code>LLM</code> –ø—É—Ç–µ–º —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. –î–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –º–æ–≥—É—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã <code>schema-based</code> –∏ <code>demonstration-based prompts</code>.</li>
                        <li><strong>Retrieval-Augmented Generation (RAG):</strong> <code>RAG</code> –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –≤—ã—Ö–æ–¥—ã <code>LLM</code>, –∏–∑–≤–ª–µ–∫–∞—è –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤. –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ, –∫–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π, —á–µ–º –º–æ–∂–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å <code>LLM</code>, –∏–ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div><div class="message ai tool-call" id="tool-SPEC_LLM-1750665329285" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
    <div class="message-avatar">
        <i class="fas fa-brain tool-icon-animation"></i>
    </div>
    <div class="message-content">
        <div class="message-header tool-header">
            <div class="tool-title">
                <i class="fas fa-brain"></i>
                <span>–ò—Å–ø–æ–ª—å–∑—É–µ–º UserCode_Assistant</span>
            </div>
            <span>10:55</span>
        </div>
        <div class="tool-description">
            Using model UserCode_Assistant for deep reasoning...
        </div>
        <div class="tool-visualization-container">
            <div class="spec-llm-output-container" style="line-height: 24px; color: rgb(255, 255, 255); font-size: 16px; --fade-end: 100%; --fade-start: 0%; max-height: 5000px;">
                <div class="spec-llm-content"><p>–î–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (fine-tuning) LLM –º–æ–¥–µ–ª–µ–π Llama 2 / Llama 3 –Ω–∞ –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ —Å RTX 4090 –∏ 128GB RAM, –≤–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ Python –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫. RTX 4090 –∏–º–µ–µ—Ç 24GB VRAM, –Ω–æ 128GB —Å–∏—Å—Ç–µ–º–Ω–æ–π RAM –ø–æ–∑–≤–æ–ª–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ offloading –∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ, –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.</p>
                    <h3>‚öôÔ∏è 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã –∏ –¥—Ä–∞–π–≤–µ—Ä–æ–≤ NVIDIA</h3>
                    <p>–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π CUDA Toolkit. RTX 4090 (Compute Capability 8.9) —Ä–∞–±–æ—Ç–∞–µ—Ç —Å CUDA 11.8 –∏ –≤—ã—à–µ, –Ω–æ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤–µ–π—à–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ PyTorch –∏ Flash Attention —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CUDA 12.x.</p>
                    <ul>
                        <li><strong>–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–æ–≤ –∏ CUDA:</strong><pre class="language-bash" tabindex="0"><code class="language-bash">nvidia-smi
</code></pre>
                            –í –≤—ã–≤–æ–¥–µ –≤—ã —É–≤–∏–¥–∏—Ç–µ –≤–µ—Ä—Å–∏—é CUDA, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é –≤–∞—à–∏–º–∏ –¥—Ä–∞–π–≤–µ—Ä–∞–º–∏.</li>
                    </ul>
                    <h3>üêç 2. –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è Python</h3>
                    <p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, <code>conda</code> –∏–ª–∏ <code>venv</code>) –∫—Ä–∞–π–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø—Ä–æ–µ–∫—Ç–∞.</p>
                    <ul>
                        <li><strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Conda (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è ML):</strong><pre class="language-bash" tabindex="0"><code class="language-bash">conda create -n llama_finetune python=3.10  # –ò–ª–∏ 3.11/3.12
conda activate llama_finetune
</code></pre>
                        </li>
                        <li><strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ venv:</strong><pre class="language-bash" tabindex="0"><code class="language-bash">python -m venv llama_finetune_env
source llama_finetune_env/bin/activate  # –î–ª—è Linux/macOS
# llama_finetune_env\Scripts\activate  # –î–ª—è Windows
</code></pre>
                        </li>
                    </ul>
                    <h3>üöÄ 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch</h3>
                    <p>PyTorch ‚Äî –æ—Å–Ω–æ–≤–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –í–∞–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–µ—Ä—Å–∏—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤–∞—à–µ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–µ –∏ –¥—Ä–∞–π–≤–µ—Ä–∞–º. –î–ª—è RTX 4090 —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA 12.x.</p>
                    <pre class="language-bash" tabindex="0"><code class="language-bash">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre>
                    <blockquote>
                        <p>üí° –ï—Å–ª–∏ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –¥—Ä—É–≥–∞—è –≤–µ—Ä—Å–∏—è CUDA, –∑–∞–º–µ–Ω–∏—Ç–µ <code>cu121</code> –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, <code>cu118</code> –¥–ª—è CUDA 11.8).</p>
                    </blockquote>
                    <h3>üì¶ 4. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Hugging Face Libraries (Transformers, PEFT, BitsAndBytes)</h3>
                    <p>–≠—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —è–≤–ª—è—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.</p>
                    <ul>
                        <li><p><strong>Transformers:</strong></p>
                            <pre class="language-bash" tabindex="0"><code class="language-bash">pip install transformers
</code></pre>
                            <blockquote>
                                <p>üìù <code>transformers</code> –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –¥–ª—è –∏—Ö —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.</p>
                            </blockquote>
                        </li>
                        <li><p><strong>PEFT (Parameter-Efficient Fine-Tuning):</strong></p>
                            <pre class="language-bash" tabindex="0"><code class="language-bash">pip install peft
</code></pre>
                            <blockquote>
                                <p>üìù PEFT (–Ω–∞–ø—Ä–∏–º–µ—Ä, LoRA, QLoRA) –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ–±–Ω–æ–≤–ª—è—è –ª–∏—à—å –º–∞–ª—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏.</p>
                            </blockquote>
                        </li>
                        <li><p><strong>Bitsandbytes:</strong></p>
                            <pre class="language-bash" tabindex="0"><code class="language-bash">pip install bitsandbytes
</code></pre>
                            <blockquote>
                                <p>üìù Bitsandbytes –Ω–µ–æ–±—Ö–æ–¥–∏–º –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ 4-–±–∏—Ç–Ω–æ–º –∏–ª–∏ 8-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å–∫–∞—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å—é, —Ç–∞–∫–æ–π –∫–∞–∫ 24GB –Ω–∞ RTX 4090.</p>
                            </blockquote>
                        </li>
                    </ul>
                    <h3>‚ö° 5. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Flash Attention (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)</h3>
                    <p>Flash Attention –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è (attention) –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è LLM.</p>
                    <pre class="language-bash" tabindex="0"><code class="language-bash"># –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω git
pip install packaging ninja
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
pip install . --no-build-isolation
cd .. # –í–µ—Ä–Ω—É—Ç—å—Å—è –≤ –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
</code></pre>
                    <blockquote>
                        <p>üí° Flash Attention 2 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Ampere, Ada (RTX 4090) –∏ Hopper GPU —Å CUDA 12.0 –∏ –≤—ã—à–µ. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö <code>pip install flash-attn</code> –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–∏–∫–æ–≤ —á–∞—Å—Ç–æ –Ω–∞–¥–µ–∂–Ω–µ–µ, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π CUDA.</p>
                    </blockquote>
                    <h3>‚úÖ 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏</h3>
                    <p>–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤—Å–µ—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é –ø—Ä–æ–≤–µ—Ä–∫—É:</p>
                    <pre class="language-python" tabindex="0"><code class="language-python">python <span class="token operator">-</span>c <span class="token string">"import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count()); print(torch.cuda.get_device_name(0))"</span>
</code></pre>
                    <p>–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å <code>True</code>, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –≤–∞—à–µ–π RTX 4090.</p>
                </div>
            </div>
        </div>
    </div>
</div><div class="message ai tool-call" id="tool-SPEC_LLM-1750665350621" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
    <div class="message-avatar">
        <i class="fas fa-brain tool-icon-animation"></i>
    </div>
    <div class="message-content">
        <div class="message-header tool-header">
            <div class="tool-title">
                <i class="fas fa-brain"></i>
                <span>–ò—Å–ø–æ–ª—å–∑—É–µ–º UserCode_Assistant</span>
            </div>
            <span>10:55</span>
        </div>
        <div class="tool-description">
            Using model UserCode_Assistant for deep reasoning...
        </div>
        <div class="tool-visualization-container">
            <div class="spec-llm-output-container" style="line-height: 24px; color: rgb(255, 255, 255); font-size: 16px; --fade-end: 100%; --fade-start: 0%; max-height: 5000px;">
                <div class="spec-llm-content"><p>–î–ª—è fine-tuning –º–æ–¥–µ–ª–∏ Llama 3 8B –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º QLoRA (4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã) —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ <code>Transformers</code>, <code>PEFT</code> –∏ <code>Bitsandbytes</code>, –≤–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥.</p>
                    <h3>‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è Fine-tuning</h3>
                    <p>–í–∞—à –¥–∞—Ç–∞—Å–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ —É—á–∏—Ç—å—Å—è –Ω–∞ –ø–∞—Ä–∞—Ö "–≤—Ö–æ–¥ ‚Äì –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ç–µ–∫—Å—Ç. –î–ª—è Llama 3, –æ—Å–æ–±–µ–Ω–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω —á–∞—Ç–∞.</p>
                    <p><strong>–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:</strong>
                        –°–æ–∑–¥–∞–π—Ç–µ JSONL-—Ñ–∞–π–ª (JSON Lines), –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–¥–∏–Ω –æ–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä. –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º —Å –∫–ª—é—á–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –î–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ fine-tuning Llama 3, —ç—Ç–æ –æ–±—ã—á–Ω–æ —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π —á–∞—Ç–∞.</p>
                    <p><strong>–ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã <code>my_dataset.jsonl</code>:</strong></p>
                    <pre class="language-json" tabindex="0"><code class="language-json"><span class="token punctuation">{</span><span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n–í—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n–ü—Ä–∏–≤–µ—Ç! –£ –º–µ–Ω—è –≤—Å–µ —Ö–æ—Ä–æ—à–æ, —Å–ø–∞—Å–∏–±–æ. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?&lt;|eot_id|&gt;"</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span><span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n–†–∞—Å—Å–∫–∞–∂–∏ –æ fine-tuning LLM.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\nFine-tuning LLM - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–µ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ –∏–ª–∏ –¥–æ–º–µ–Ω—É.&lt;|eot_id|&gt;"</span><span class="token punctuation">}</span>
</code></pre>
                    <blockquote>
                        <p>üí° <strong>–í–∞–∂–Ω–æ:</strong> Llama 3 Instruct –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —à–∞–±–ª–æ–Ω (<code>&lt;|begin_of_text|&gt;</code>, <code>&lt;|start_header_id|&gt;</code>, <code>&lt;|end_header_id|&gt;</code>, <code>&lt;|eot_id|&gt;</code>) –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ <code>tokenizer.apply_chat_template()</code> –ø–æ–º–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.</p>
                    </blockquote>
                    <h3>üìù –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è Fine-tuning</h3>
                    <p>–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: <code>transformers</code>, <code>peft</code>, <code>bitsandbytes</code>, <code>accelerate</code>, <code>trl</code>, <code>datasets</code>.</p>
                    <pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> <span class="token punctuation">(</span>
    AutoModelForCausalLM<span class="token punctuation">,</span>
    AutoTokenizer<span class="token punctuation">,</span>
    BitsAndBytesConfig<span class="token punctuation">,</span>
    TrainingArguments<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> prepare_model_for_kbit_training
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> trl <span class="token keyword">import</span> SFTTrainer
<span class="token keyword">import</span> os

<span class="token comment"># 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</span>
model_id <span class="token operator">=</span> <span class="token string">"meta-llama/Meta-Llama-3-8B-Instruct"</span> <span class="token comment"># –ò–ª–∏ 'meta-llama/Meta-Llama-3-8B' –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏</span>
dataset_path <span class="token operator">=</span> <span class="token string">"my_dataset.jsonl"</span> <span class="token comment"># –ü—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞</span>
output_dir <span class="token operator">=</span> <span class="token string">"./llama3_finetuned"</span>
lora_r <span class="token operator">=</span> <span class="token number">16</span> <span class="token comment"># –†–∞–Ω–≥ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ [4, 5]</span>
lora_alpha <span class="token operator">=</span> <span class="token number">16</span> <span class="token comment"># –ü–∞—Ä–∞–º–µ—Ç—Ä –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LoRA [4]</span>
lora_dropout <span class="token operator">=</span> <span class="token number">0.05</span> <span class="token comment"># Dropout –¥–ª—è LoRA —Å–ª–æ–µ–≤ [4]</span>
max_seq_length <span class="token operator">=</span> <span class="token number">1024</span> <span class="token comment"># –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –£–≤–µ–ª–∏—á—å—Ç–µ, –µ—Å–ª–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–µ—Å—É—Ä—Å—ã –∏ –¥–∞–Ω–Ω—ã–µ. [5]</span>
num_train_epochs <span class="token operator">=</span> <span class="token number">3</span> <span class="token comment"># –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è [8]</span>
per_device_train_batch_size <span class="token operator">=</span> <span class="token number">4</span> <span class="token comment"># –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ GPU. Adjust based on VRAM.</span>
gradient_accumulation_steps <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment"># –®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞</span>
learning_rate <span class="token operator">=</span> <span class="token number">2e-4</span>
<span class="token comment"># –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: Flash Attention 2 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è, –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è GPU –∏ CUDA.</span>
<span class="token comment"># from transformers.integrations import is_flash_attn_2_available</span>
<span class="token comment"># use_flash_attention_2 = is_flash_attn_2_available()</span>

<span class="token comment"># 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞..."</span><span class="token punctuation">)</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"json"</span><span class="token punctuation">,</span> data_files<span class="token operator">=</span>dataset_path<span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>

<span class="token comment"># –ï—Å–ª–∏ –≤–∞—à –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç, –≤–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç —á–∞—Ç–∞ Llama 3</span>
<span class="token comment"># –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ –≤–∞—à JSONL –∏–º–µ–µ—Ç –∫–ª—é—á–∏ 'instruction' –∏ 'response'):</span>
<span class="token comment"># def formatting_prompts_func(example):</span>
<span class="token comment">#     output_texts = []</span>
<span class="token comment">#     for i in range(len(example['instruction'])):</span>
<span class="token comment">#         messages = [</span>
<span class="token comment">#             {"role": "user", "content": example['instruction'][i]},</span>
<span class="token comment">#             {"role": "assistant", "content": example['response'][i]},</span>
<span class="token comment">#         ]</span>
<span class="token comment">#         output_texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))</span>
<span class="token comment">#     return {"text": output_texts}</span>
<span class="token comment"># dataset = dataset.map(formatting_prompts_func, batched=True)</span>


<span class="token comment"># 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (4-bit)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ 4-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è..."</span><span class="token punctuation">)</span>
bnb_config <span class="token operator">=</span> BitsAndBytesConfig<span class="token punctuation">(</span>
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ [1, 9]</span>
    bnb_4bit_use_double_quant<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–µ–π –ø–∞–º—è—Ç–∏ [5]</span>
    bnb_4bit_quant_type<span class="token operator">=</span><span class="token string">"nf4"</span><span class="token punctuation">,</span> <span class="token comment"># –¢–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è: NF4 (NormalFloat 4-bit) [1, 5]</span>
    bnb_4bit_compute_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span> <span class="token comment"># –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ bfloat16 –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π [5]</span>
<span class="token punctuation">)</span>

<span class="token comment"># 4. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ </span><span class="token interpolation"><span class="token punctuation">{</span>model_id<span class="token punctuation">}</span></span><span class="token string"> —Å 4-–±–∏—Ç–Ω—ã–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º..."</span></span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span>
    quantization_config<span class="token operator">=</span>bnb_config<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> <span class="token comment"># –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span> <span class="token comment"># –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ bfloat16 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è GPU (Ampere+)</span>
    <span class="token comment"># attn_implementation="flash_attention_2" if use_flash_attention_2 else "eager", # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: Flash Attention 2</span>
<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_cache <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># –û—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à –¥–ª—è –æ–±—É—á–µ–Ω–∏—è</span>
model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment"># –£–º–µ–Ω—å—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏</span>

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token <span class="token comment"># –£—Å—Ç–∞–Ω–æ–≤–∫–∞ pad_token</span>
tokenizer<span class="token punctuation">.</span>padding_side <span class="token operator">=</span> <span class="token string">"right"</span> <span class="token comment"># Padding —Å–ø—Ä–∞–≤–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ Llama [5]</span>

<span class="token comment"># 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è k-bit –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤..."</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> prepare_model_for_kbit_training<span class="token punctuation">(</span>model<span class="token punctuation">)</span> <span class="token comment"># –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é [5]</span>

peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    lora_alpha<span class="token operator">=</span>lora_alpha<span class="token punctuation">,</span>
    lora_dropout<span class="token operator">=</span>lora_dropout<span class="token punctuation">,</span>
    r<span class="token operator">=</span>lora_r<span class="token punctuation">,</span>
    bias<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">,</span>
    task_type<span class="token operator">=</span><span class="token string">"CAUSAL_LM"</span><span class="token punctuation">,</span>
    <span class="token comment"># –¶–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ LoRA: –ø—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤/–∫–ª—é—á–µ–π/–∑–Ω–∞—á–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –∞–ø—Å–µ–º–ø–ª–∏–Ω–≥–∞/–¥–∞—É–Ω—Å–µ–º–ø–ª–∏–Ω–≥–∞</span>
    target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">,</span> <span class="token string">"k_proj"</span><span class="token punctuation">,</span> <span class="token string">"o_proj"</span><span class="token punctuation">,</span> <span class="token string">"gate_proj"</span><span class="token punctuation">,</span> <span class="token string">"up_proj"</span><span class="token punctuation">,</span> <span class="token string">"down_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># –í—ã–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç—Ä–µ–Ω–∏—Ä—É–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</span>

<span class="token comment"># 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è..."</span><span class="token punctuation">)</span>
training_arguments <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span>output_dir<span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span>num_train_epochs<span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span>per_device_train_batch_size<span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span>gradient_accumulation_steps<span class="token punctuation">,</span>
    optim<span class="token operator">=</span><span class="token string">"paged_adamw_8bit"</span><span class="token punctuation">,</span> <span class="token comment"># –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è 8-–±–∏—Ç–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ [1]</span>
    save_steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token comment"># –°–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment"># –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤</span>
    learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span>
    weight_decay<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment"># –û—Ç–∫–ª—é—á–∏—Ç—å fp16, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º bfloat16 –¥–ª—è compute_dtype</span>
    bf16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bfloat16</span>
    max_grad_norm<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token comment"># –û–±—Ä–µ–∑–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞</span>
    max_steps<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># –û–±—É—á–∞—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º, –∞ –Ω–µ –ø–æ —à–∞–≥–∞–º</span>
    warmup_ratio<span class="token operator">=</span><span class="token number">0.03</span><span class="token punctuation">,</span> <span class="token comment"># –î–æ–ª—è —à–∞–≥–æ–≤ –¥–ª—è —Ä–∞–∑–æ–≥—Ä–µ–≤–∞ learning rate</span>
    lr_scheduler_type<span class="token operator">=</span><span class="token string">"cosine"</span><span class="token punctuation">,</span> <span class="token comment"># –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate</span>
    <span class="token comment"># group_by_length=True, # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ –¥–ª–∏–Ω–µ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –±–∞—Ç—á–∏–Ω–≥–∞</span>
    report_to<span class="token operator">=</span><span class="token string">"tensorboard"</span><span class="token punctuation">,</span> <span class="token comment"># –û—Ç—á–µ—Ç—ã –≤ TensorBoard</span>
<span class="token punctuation">)</span>

<span class="token comment"># 7. –ó–∞–ø—É—Å–∫ SFTTrainer</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SFTTrainer..."</span><span class="token punctuation">)</span>
trainer <span class="token operator">=</span> SFTTrainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>dataset<span class="token punctuation">,</span>
    peft_config<span class="token operator">=</span>peft_config<span class="token punctuation">,</span>
    dataset_text_field<span class="token operator">=</span><span class="token string">"text"</span><span class="token punctuation">,</span> <span class="token comment"># –ò–º—è –ø–æ–ª—è –≤ –≤–∞—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ —Ç–µ–∫—Å—Ç</span>
    max_seq_length<span class="token operator">=</span>max_seq_length<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_arguments<span class="token punctuation">,</span>
    packing<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token comment"># True –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ "—É–ø–∞–∫–æ–≤–∫–µ"</span>
<span class="token punctuation">)</span>

<span class="token comment"># 8. –û–±—É—á–µ–Ω–∏–µ</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è..."</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤..."</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span>output_dir<span class="token punctuation">)</span>

<span class="token comment"># –ß—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å + –∞–¥–∞–ø—Ç–µ—Ä—ã), –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö.</span>
<span class="token comment"># –≠—Ç–æ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ VRAM.</span>
<span class="token comment"># from peft import AutoPeftModelForCausalLM</span>
<span class="token comment"># merged_model_path = "./llama3_finetuned_merged"</span>
<span class="token comment"># print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {merged_model_path}...")</span>
<span class="token comment"># loaded_model = AutoPeftModelForCausalLM.from_pretrained(</span>
<span class="token comment">#     output_dir,</span>
<span class="token comment">#     low_cpu_mem_usage=True,</span>
<span class="token comment">#     torch_dtype=torch.bfloat16,</span>
<span class="token comment"># )</span>
<span class="token comment"># loaded_model = loaded_model.merge_and_unload()</span>
<span class="token comment"># loaded_model.save_pretrained(merged_model_path)</span>
<span class="token comment"># tokenizer.save_pretrained(merged_model_path)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Fine-tuning –∑–∞–≤–µ—Ä—à–µ–Ω–æ!"</span><span class="token punctuation">)</span>
</code></pre>
                    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è:</h3>
                    <ul>
                        <li><strong>Bitsandbytes –¥–ª—è 4-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (QLoRA):</strong> <code>BitsAndBytesConfig</code> –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ 4-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (<code>load_in_4bit=True</code>), —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM. <code>bnb_4bit_quant_type="nf4"</code> –∏ <code>bnb_4bit_compute_dtype=torch.bfloat16</code> ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Llama 3 –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU (NVIDIA Ampere –∏–ª–∏ Ada Lovelace, –∫ –∫–æ—Ç–æ—Ä–æ–π –æ—Ç–Ω–æ—Å–∏—Ç—Å—è RTX 4090).</li>
                        <li><strong>PEFT (LoRA):</strong> <code>LoraConfig</code> –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ LoRA.<ul>
                            <li><code>r</code> (—Ä–∞–Ω–≥): –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã—Ö –º–∞—Ç—Ä–∏—Ü. –ë–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ <code>r</code> —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–Ω–∏—Ä—É–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏.</li>
                            <li><code>lora_alpha</code>: –ü–∞—Ä–∞–º–µ—Ç—Ä –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.</li>
                            <li><code>target_modules</code>: –°–ø–∏—Å–æ–∫ —Å–ª–æ–µ–≤, –∫ –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã. –î–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Llama —Ç–∏–ø–∏—á–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –≤–∫–ª—é—á–∞—é—Ç –ø—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ (<code>q_proj</code>), –∫–ª—é—á–µ–π (<code>k_proj</code>), –∑–Ω–∞—á–µ–Ω–∏–π (<code>v_proj</code>), –≤—ã–≤–æ–¥–∞ (<code>o_proj</code>), –∞ —Ç–∞–∫–∂–µ <code>gate_proj</code>, <code>up_proj</code> –∏ <code>down_proj</code> –≤ MLP-–±–ª–æ–∫–∞—Ö.</li>
                            <li><code>prepare_model_for_kbit_training()</code>: –§—É–Ω–∫—Ü–∏—è –∏–∑ PEFT, –∫–æ—Ç–æ—Ä–∞—è –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã.</li>
                        </ul>
                        </li>
                        <li><strong>Hugging Face <code>Trainer</code> –∏ <code>SFTTrainer</code>:</strong><ul>
                            <li><code>TrainingArguments</code>: –ö–ª–∞—Å—Å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, learning rate, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏ —Ç.–¥. –î–ª—è QLoRA —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è <code>paged_adamw_8bit</code> –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä.</li>
                            <li><code>SFTTrainer</code> (Supervised Fine-tuning Trainer) –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ <code>trl</code> (Transformer Reinforcement Learning) —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–ª–∏ –¥–∏–∞–ª–æ–≥–æ–≤. –û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ <code>dataset_text_field</code> –ø—Ä–∞–≤–∏–ª—å–Ω–æ —É–∫–∞–∑–∞–Ω.</li>
                        </ul>
                        </li>
                        <li><strong>–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞:</strong> –î–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Llama 3 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ —à–∞–±–ª–æ–Ω—É, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞. <code>tokenizer.apply_chat_template</code> ‚Äî –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è <code>Llama 3 Instruct</code>.</li>
                        <li><strong>RTX 4090 –∏ 128GB RAM:</strong> –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤ 4 –±–∏—Ç–∞ (<code>BitsAndBytesConfig</code>) –∏ LoRA (<code>PEFT</code>) —è–≤–ª—è—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã Llama 3 8B –ø–æ–º–µ—Å—Ç–∏–ª–∞—Å—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –≤–∞—à–µ–π RTX 4090 (24GB VRAM). 128GB —Å–∏—Å—Ç–µ–º–Ω–æ–π RAM –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∑–∞–ø–∞—Å –¥–ª—è <code>offloading</code> –∏ –¥—Ä—É–≥–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏, —á–µ–º VRAM.</li>
                    </ul>
                    <p>–≠—Ç–æ—Ç –∫–æ–¥ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Å–Ω–æ–≤—É –¥–ª—è fine-tuning Llama 3. –í–∞–º –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, <code>lora_r</code>, <code>lora_alpha</code>, <code>learning_rate</code>, <code>num_train_epochs</code>, <code>per_device_train_batch_size</code>) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.</p>
                </div>
            </div>
        </div>
    </div>
</div><div class="message ai tool-call" id="tool-SPEC_LLM-1750665381748" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
    <div class="message-avatar">
        <i class="fas fa-brain tool-icon-animation"></i>
    </div>
    <div class="message-content">
        <div class="message-header tool-header">
            <div class="tool-title">
                <i class="fas fa-brain"></i>
                <span>–ò—Å–ø–æ–ª—å–∑—É–µ–º Conv_Philosophic</span>
            </div>
            <span>10:56</span>
        </div>
        <div class="tool-description">Using model gpt-4.5-preview-2025-02-27 for deep reasoning...</div>
        <div class="tool-visualization-container">
            <div class="spec-llm-output-container">
                <div class="spec-llm-content"><p>–ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ fine-tuning LLM –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∏—á–µ—Å–∫–∏–º –Ω–æ—Ä–º–∞–º –∏ –Ω–æ—Ä–º–∞–º –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏, —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è —Å–ª–µ–¥—É—é—â–∏—Ö –º–µ—Ä –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:</p>
                    <h3>1. –ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è –∏ –¥–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö</h3>
                    <ul>
                        <li>–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–∏–º–µ–Ω–∞, –∞–¥—Ä–µ—Å–∞, –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ —Ç.–¥.) –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω—ã –∏–ª–∏ –Ω–∞–¥–µ–∂–Ω–æ –æ–±–µ–∑–ª–∏—á–µ–Ω—ã.</li>
                        <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, NLP-–º–æ–¥–µ–ª–∏ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è) –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>
                    </ul>
                    <h3>2. –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∏ —Å–æ–≥–ª–∞—Å–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π</h3>
                    <ul>
                        <li>–ß–µ—Ç–∫–æ –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ —Ç–æ–º, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏—Ö –¥–∞–Ω–Ω—ã–µ –∏ –∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ —Ä–µ—à–∞–µ—Ç –º–æ–¥–µ–ª—å.</li>
                        <li>–ü–æ–ª—É—á–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–ª–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.</li>
                    </ul>
                    <h3>3. –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö</h3>
                    <ul>
                        <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ç–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ü–µ–ª–∏.</li>
                        <li>–ò–∑–±–µ–≥–∞–π—Ç–µ —Å–±–æ—Ä–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö.</li>
                    </ul>
                    <h3>4. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª–∏</h3>
                    <ul>
                        <li>–°—Ç—Ä–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á—å—Ç–µ –∫—Ä—É–≥ –ª–∏—Ü, –∏–º–µ—é—â–∏—Ö –¥–æ—Å—Ç—É–ø –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∏ fine-tuned –º–æ–¥–µ–ª—è–º.</li>
                        <li>–í–Ω–µ–¥—Ä–∏—Ç–µ –Ω–∞–¥–µ–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ä—ã –∑–∞—â–∏—Ç—ã (—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞, –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞).</li>
                    </ul>
                    <h3>5. –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –∏ —ç—Ç–∏—á–µ—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞</h3>
                    <ul>
                        <li>–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ä–∏—Å–∫–æ–≤ (privacy impact assessment, ethical audit), —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —ç—Ç–∏—á–µ—Å–∫–∏–µ –∏ –ø—Ä–∞–≤–æ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.</li>
                        <li>–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–≤–ª–µ–∫–∏—Ç–µ –≤–Ω–µ—à–Ω–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –æ—Ü–µ–Ω–∫–∏.</li>
                    </ul>
                    <h3>6. –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–¥–∞ –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–∏</h3>
                    <ul>
                        <li>–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–º–µ—â–µ–Ω–∏–π (bias) –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–∏ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –≥—Ä—É–ø–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.</li>
                        <li>–†–µ–≥—É–ª—è—Ä–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏–ª–∏ –≤—Ä–µ–¥–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.</li>
                    </ul>
                    <h3>7. –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</h3>
                    <ul>
                        <li>–ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±–µ—Å–ø–µ—á—å—Ç–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏.</li>
                        <li>–†–∞–∑—Ä–∞–±–æ—Ç–∞–π—Ç–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–º, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª—å —Å–¥–µ–ª–∞–ª–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤—ã–≤–æ–¥.</li>
                    </ul>
                    <h3>8. –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏ –∞—É–¥–∏—Ç</h3>
                    <ul>
                        <li>–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—É—â–∏–º —ç—Ç–∏—á–µ—Å–∫–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∏ –Ω–æ—Ä–º–∞–º –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.</li>
                        <li>–°–æ–∑–¥–∞–π—Ç–µ –º–µ—Ö–∞–Ω–∏–∑–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ —Ä–µ–∞–≥–∏—Ä—É–π—Ç–µ –Ω–∞ –∂–∞–ª–æ–±—ã –∏–ª–∏ —Å–∏–≥–Ω–∞–ª—ã –æ –Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö.</li>
                    </ul>
                    <h3>9. –°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã–º –Ω–æ—Ä–º–∞–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º</h3>
                    <ul>
                        <li>–°–æ–±–ª—é–¥–∞–π—Ç–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ GDPR (EU), CCPA (–°–®–ê), –§–ó-152 (–†–§) –∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –≤ –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω–∞—Ö.</li>
                        <li>–°–ª–µ–¥—É–π—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –ø–æ —ç—Ç–∏—á–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç IEEE, UNESCO, OECD).</li>
                    </ul>
                    <p>–ü—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—è—Å—å —ç—Ç–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –≤—ã —Å–º–æ–∂–µ—Ç–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ LLM –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –±–µ–∑ –Ω–∞—Ä—É—à–µ–Ω–∏—è —ç—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–æ—Ä–º –∏ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.</p>
                </div>
            </div>
        </div>
    </div>
</div><div class="message ai" id="msg-1750665416772-fwdplo" style="translate: none; rotate: none; scale: none; transform: translate(0px, 0px); opacity: 1;">
    <div class="message-avatar">
        <i class="custom-icon-mask"></i>
    </div>
    <div class="message-content">
        <div class="message-text"><p><code>&lt;thinking&gt;</code>–ó–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ fine-tuning LLM –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ (—Å–∞–º–æ–≥–æ —Å–µ–±—è) —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º —ç—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–æ—Ä–º. –Ø —Ä–∞–∑–±–∏–ª –∑–∞–¥–∞—á—É –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤: 1. –í—ã–±–æ—Ä –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è fine-tuning. 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞. 3. Fine-tuning –º–æ–¥–µ–ª–∏. 4. –û—Ü–µ–Ω–∫–∞ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. 5. –≠—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ –º–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏. –ù–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–∞—Ö —è –ø–æ–ª—É—á–∏–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –º–æ–¥–µ–ª—è—Ö, –æ–∫—Ä—É–∂–µ–Ω–∏–∏, –∫–æ–¥–µ fine-tuning –∏ —ç—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –¢–µ–ø–µ—Ä—å —è –º–æ–≥—É —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø–ª–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.</p>
            <ol>
                <li><strong>–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è:</strong> –í—ã–±–µ—Ä—É Llama 3 8B Instruct. –ü–æ–¥–≥–æ—Ç–æ–≤–ª—é –æ–∫—Ä—É–∂–µ–Ω–∏–µ Python —Å PyTorch, Transformers, PEFT, Bitsandbytes, Flash Attention.</li>
                <li><strong>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:</strong>  –°–æ–±–µ—Ä—É –¥–∞–Ω–Ω—ã–µ –æ —Å–≤–æ–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö (–≤–æ–ø—Ä–æ—Å—ã, —Ä–µ—à–µ–Ω–∏—è). –ü–æ–¥–≥–æ—Ç–æ–≤–ª—é JSONL —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ–¥—Ö–æ–¥—è—â–µ–º –¥–ª—è Llama 3 Instruct (–∏—Å–ø–æ–ª—å–∑—É—é tokenizer.apply_chat_template). –£–±–µ–∂—É—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.</li>
                <li><strong>Fine-tuning:</strong> –ò—Å–ø–æ–ª—å–∑—É—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è fine-tuning Llama 3 8B —Å QLoRA.  –ù–∞—Å—Ç—Ä–æ—é –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (lora_r, lora_alpha, learning_rate, num_train_epochs, per_device_train_batch_size).</li>
                <li><strong>–û—Ü–µ–Ω–∫–∞:</strong> –û—Ü–µ–Ω—é –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (–æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ).  –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö.</li>
                <li><strong>–≠—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞:</strong> –ü—Ä–æ–≤–µ—Ä—é –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–º–µ—â–µ–Ω–∏–π. –£–±–µ–∂—É—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–¥–∞–µ—Ç –≤—Ä–µ–¥–Ω—ã–µ –∏–ª–∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã.</li>
                <li><strong>–ò—Ç–µ—Ä–∞—Ü–∏–∏:</strong> –ë—É–¥—É –ø–æ–≤—Ç–æ—Ä—è—Ç—å —à–∞–≥–∏ 2-5, —É–ª—É—á—à–∞—è –¥–∞—Ç–∞—Å–µ—Ç, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —ç—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏.</li>
            </ol>
            <p>–ù–∞—á–∞—Ç—å —Å–ª–µ–¥—É–µ—Ç —Å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞, —Ç.–∫. –æ—Ç —ç—Ç–æ–≥–æ –∑–∞–≤–∏—Å–∏—Ç —É—Å–ø–µ—Ö –≤—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.
                <code>&lt;/thinking&gt;</code></p>
        </div>
    </div>
</div></div>
